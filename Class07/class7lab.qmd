---
title: "Class 7: Introduction to machine learning for Bioinformatics"
author: "Libby Gilmore pid: A69047570"
format: pdf
editor: visual
---

```{r}
x <- c(rnorm(30,mean=-3), rnorm(30, mean=3))
y <- rev(x) # reverses the argument; so y should be +3 for 30 then -3 for next 30

x <- cbind(x,y)
head(x)
```

```{r}
# a look at x with `plot()`
plot(x)

```

The main function in "base" R for K-means clustering is called `kmeans()`

```{r}
# kmeans has two required arguments, data, and centers 
#(centers is k, the amount of clusters you want)
k <- kmeans(x,centers=2) # these cluster means end up at ~+/- 3
# clustering vector: is a vector of integers from 1:k indicating the cluster 
# to which each point is allocated
k
```

> Q. How big are the clusters (i.e. their size)?

```{r}
k$size
```

> Q. What clusters do my data points reside in?

```{r}
# A vector of integers (from 1:k) indicating the cluster to which each point is allocated.
k$cluster
```

> Q. Make a plot of our data colored by cluster assignment -- i.e. Make a result figure...

```{r}
plot(x, col=k$cluster)
points(k$centers, col="blue", pch=15)
# plot(x, col=2) # you can also color by number

```

> Q. Cluster with kmeans into 4 clusters and plot your results above

```{r}
k4 <- kmeans(x, centers=4)
plot(x, col=k4$cluster)
points(k4$centers, col="blue", pch=15)

```

> Q. Run kmeans with center (i.e values of k) equal to 1 to 6

```{r}
k$tot.withinss # sum of squares you want to store those values for all of sum of centers; and I want to plot this against each k

k1 <- kmeans(x,centers=1)$tot.withinss
k2 <- kmeans(x,centers=2)$tot.withinss
k3 <- kmeans(x,centers=3)$tot.withinss
k4 <- kmeans(x,centers=4)$tot.withinss
k5 <- kmeans(x,centers=5)$tot.withinss
k6 <- kmeans(x,centers=6)$tot.withinss

```

```{r}
ans <- NULL
for(i in 1:6){
  # cat(i) to check each integer is being incremented
  ans <- c(ans, kmeans(x, centers = i)$tot.withinss)
}
ans
```

```{r}
# Make a scree-plot, used for PCAs, you get the most bang for your buck when k is 2. After that it becomes inconsequential
plot(ans, typ="b")
```

## Hierarchical Clustering

The main function in base R for this is called `hclust()`

```{r}
# hclust(x)
d <- dist(x)
hc <- hclust(d) # input needs to be a distance matrix
hc
```

you can just call `plot()` on your hc object. It produces a cluster dendrogram, and the ordering in this labeling is random, but what does matter is the bar lengths (height)

```{r}
plot(hc)
abline(h=7,col="red")
```

To obtain clusters from out `hclust` object **hc** we "cut" the tree to yield different sub branches. For this we use `cutree()` function.

```{r}
grps <- cutree(hc, h=7)
grps
```

```{r}
plot(x, col=grps)
```

```{r}
library(pheatmap)
pheatmap(x)
```

### Principal Component Analysis

```{r}
# data import
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
# View(x)
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```

```{r}
## Preview the first 6 rows
head(x)
```

```{r}
# Note how the minus indexing works
rownames(x) <- x[,1] # sets rownames to first column
x <- x[,-1] # everything but the first column; thereby removes the first column
head(x)
```

```{r}
dim(x) # it has one less column
```

```{r}
# An alternative approach to setting the correct row-names when importing dataset
x <- read.csv(url, row.names=1)
head(x)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

`read.csv(url, row.names=1)` is more robust under certain circumstances; whereas `x<-x[,-1]` will keep removing columns

```{r}
# Using base R
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x))) # effectively makes a new color for each food group
```

> Q3. Changing what optional argument in the above barplot() function results in the following plot?

```{r}
# change beside argument to false
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

```{r}
# Currently we have wide format
dim(x)
head(x)
```

```{r}
library(tidyr)

# Convert data to long format for gpglot with `pivot_longer()`

x_long <- x |> 
  tibble::rownames_to_column("Food") |> 
  pivot_longer(cols = -Food,
               names_to = "Country",
               values_to = "Consumption")

dim(x_long)
```

```{r}
head(x_long)
```

```{r}
# Create grouped bar plot
library(ggplot2)

ggplot(x_long) +
  aes(x = Country, y = Consumption, fill = Food) +
  geom_col(position = "dodge") +
  theme_bw()
```

> Q4: Changing what optional argument in the above ggplot() code results in a stacked barplot figure?

```{r}
ggplot(x_long) +
  aes(x = Country, y = Consumption, fill = Food) +
  geom_col() + # no more position = "dodge"
  theme_bw()
```

## Pairs plots and heatmaps

> Q5: We can use the pairs() function to generate all pairwise plots for our countries. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)

# if a given point lies along the diagonal for a given plot it means those two countries have similar consumption habits for that food group, high similarity

# England is all in the y-axis of the top row, and the x-axis of the first column
```

```{r}
library(pheatmap)

pheatmap( as.matrix(x) )
```

> Q6. Based on the pairs and heatmap figures, which countries cluster together and what does this suggest about their food consumption patterns? Can you easily tell what the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

```{r}
# You can see more dissimilarities in N. Ireland in comparison to Scotland, England, and Wales; but the colors with the scale make it hard to definitively interpret them aside from alcoholic drinks, fresh potatoes, and fresh fruit differences. 
```

## PCA to the rescue

The main function in "base" R for PCA is called `prcomp()`

As we want to do PCA on the food data for the different countries we will want the goods in the columns.

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) ) # so we transpose it to get foods in columns, otherwise we do the wrong analysis.
summary(pca)
```

Our result object is called `pca` and it has a `$x` component that we will look at first

```{r}
pca$x
```

> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# Create a data frame for plotting
df <- as.data.frame(pca$x)
df$Country <- rownames(df)

# Plot PC1 vs PC2 with ggplot
ggplot(pca$x) +
  aes(x = PC1, y = PC2, label = rownames(pca$x)) +
  geom_point(size = 3) +
  geom_text(vjust = -0.5) +
  xlim(-270, 500) +
  xlab("PC1 67.44%") +
  ylab("PC2 29.05%") +
  theme_bw()
```

> Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
cols <- c("orange", "red", "blue", "darkgreen")

# Plot PC1 vs PC2 with ggplot
ggplot(pca$x) +
  aes(x = PC1, y = PC2, label = rownames(pca$x), col=cols) +
  geom_point(size = 3) +
  geom_text(vjust = -0.5) +
  xlim(-270, 500) +
  xlab("PC1 67.44%") +
  ylab("PC2 29.05%") +
  theme_bw()
```

Another major results of PCA is the so-called "variable loadings" or `$rotation` that tells us how the original variables (foods) contribute to PCs (i.e. our new axis)

> Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
# remember since N. Ireland is on the plus size, so the things that make it more distinct are soft_drinks, fresh_potatoes, and carcass_meat
ggplot(pca$rotation) +
  aes(PC1, rownames(pca$rotation)) +
  geom_col()
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
## or the second row here...
z <- summary(pca)
z$importance
```

```{r}
# Create a scree plot
variance_df <- data.frame(
  PC = factor(paste0("PC", 1:length(v)), levels = paste0("PC", 1:length(v))),
  Variance = v
)

ggplot(variance_df) +
  aes(x = PC, y = Variance) +
  geom_col(fill = "steelblue") +
  xlab("Principal Component") +
  ylab("Percent Variation") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 0))
```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
ggplot(pca$rotation) +
  aes(x = PC1, 
      y = reorder(rownames(pca$rotation), PC1)) +
  geom_col(fill = "steelblue") +
  xlab("PC1 Loading Score") +
  ylab("") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 9))
```
